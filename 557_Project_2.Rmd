---
title: "557_Project_2BS"
author: "Ben Straub, Hillary Koch, Jiawei Huang, Arif Masrur"
date: "3/15/2017"
output: pdf_document
---

# No Command Lines Ever.  Whoa

```{r, echo=FALSE, warning=FALSE, message=FALSE}
c# Configuring Space
rm(list=ls())
par(mfrow=c(1,1))

# Loading packages into R
library(data.table);library(car);library(lars);library(knitr);library(ISLR);library(leaps);library(glmnet);library(MASS);library(reshape);library(ggplot2)

# Loading up files
setwd("/Users/benStraub/Desktop/557/Project 2")
seismic <- read.csv("seismic.csv")
```

## What the Factor Variables look like

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
par(mfrow=c(2,2))

# Barplots of Factor Variables
counts <- table(seismic$class)
barplot(counts, main="Class/Response Distribution", 
  	xlab="Number of Obs")
counts <- table(seismic$seismic)
barplot(counts, main="Seismic Distribution", 
  	xlab="Number of Obs")
counts <- table(seismic$seismoacoustic)
barplot(counts, main="Seismoacoustic Distribution", 
  	xlab="Number of Obs")
counts <- table(seismic$ghazard)
barplot(counts, main="Ghazard Distribution", 
  	xlab="Number of Obs")
```

## What the Continuous Variables look like

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA, cache=TRUE}
par(mfrow=c(2,2))

## Have a look at the densities

plot(density(seismic$genergy));plot(density(seismic$gpuls))

#plot(density(log(seismic$genergy)));plot(density(log(seismic$gpuls)))

plot(density(seismic$gdenergy));plot(density(seismic$gdpuls))
plot(density(seismic$maxenergy));plot(density(seismic$nbumps, adjust=10))
plot(density(seismic$nbumps2,adjust=10));plot(density(seismic$nbumps3,adjust=10))
plot(density(seismic$nbumps4,adjust=10));plot(density(seismic$nbumps5,adjust=10))

##---------------------------------------------
## Some quick EDA from Hillary
##---------------------------------------------

## Normalize things that arent factors
## Eliminate data that has no info (some of the nbumps)

seismic[,c(4:7,9:13,17:18)] <- seismic[,c(4:7,9:13,17:18)]
seismic <- seismic[,-(14:16)]
#seismic$class <- as.factor(seismic$class)

for(i in c(1:3,8)){
  seismic[,i] <- as.numeric(seismic[,i])
}

fit <- lm(class~., data = seismic)
summary(fit)

#pairs(seismic)

## qqplots, except for factors
for(i in c(4:7,9:15)){
  eval(parse(text = paste0("qqnorm(seismic$",names(seismic)[i],")")))
  eval(parse(text = paste0("qqline(seismic$",names(seismic)[i],", col = 2)")))
}

## Check out the residuals
res <- fit$residuals
fitvals <- fit$fitted
plot(fitvals, res, xlab = "Fitted Values", ylab = "Residuals")
abline(h=0, col = 'red')
hist(res, xlab = "Residuals")
```

## Lots of multicollinearity to worry about during variable selection

```{r}
vif(fit)
```

# Correlation of the Variables 

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA, cache=TRUE}
# Correlation Plot of the data...7 variables removed
source("http://www.sthda.com/upload/rquery_cormat.r")
x <- seismic[c(-1,-2,-3,-8,-14,-15,-16,-19)]
require("corrplot")
rquery.cormat(x, graphType = "heatmap")
rquery.cormat(x, type="flatten")

# Not sure how to suppress the the output.  Just want the plot.

#rquery.cormat(x, type="flatten", graph=FALSE)
```

# Separating into Test and Training Sets

```{r, echo=TRUE, warning=FALSE, message=FALSE, comment=NA}
##------------------------------------
## Setting up Test and Training Sets
##------------------------------------

n <- dim(seismic)[1]
p <- dim(seismic)[2]

set.seed(2016)
test <- sample(n, round(n/4))
train <- (1:n)[-test]
seismic.train <- seismic[train,]
seismic.test <- seismic[test,]

dim(seismic)
dim(seismic.train)
dim(seismic.test)

#View(seismic.train)
#View(seismic.test)
```

# Linear regression of an indicator matrix

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
##-----------------------------------------------------
## Linear regression of an indicator matrix Part I (Train)
##-----------------------------------------------------

# Setting Graphics parameters
par(mfrow=c(1,1))

## Getting data ready to perform pca/indactor matrix
RawData.train <- seismic.train
#RawData$seismic <- as.integer(RawData$seismic  == "a")
#RawData.train <- RawData.train[c(-1,-2,-3,-8,-14,-15,-16)]#getting rid of factor variable for now, nbumps greater than five have almost entirely 0 columns

responseY <- RawData.train[16]#selecting our response of interest
predictorX <- RawData.train[,1:15]

#running pca
pc.comp <- princomp(scale(predictorX)) 

#  for pc
# pc.comp$loadings

#Creates screeplot to figure out which pc's to include
screeplot(pc.comp, type="lines")
pc.comp <- princomp(scale(predictorX))$scores 
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
##-----------------------------------------------------
## Linear regression of an indicator matrix Part II (Train)
##-----------------------------------------------------


par(mfrow=c(1,1))

#Based on scree plot im going to go with 3 pc scores
pc.comp1 <- pc.comp[,1] 
pc.comp2 <- pc.comp[,2] 
pc.comp3 <- pc.comp[,3] 
X <- cbind(1,pc.comp1, pc.comp2,pc.comp3)


# Following LeBao's code
class1 <- which(responseY==1) 
class2 <- which(responseY==0) 
Y <- matrix(0,dim(responseY)[1],2) 
Y[class2,1] <- 1 
Y[class1,2] <- 1

betaHat <- solve(t(X)%*%X)%*%t(X)%*%Y 
Y1 <- X%*%betaHat[,1] 
Y2 <- X%*%betaHat[,2]

# Plots the pca scores...doesn't look good.
plot(pc.comp1[class1],pc.comp2[class1],main="PC1 vs PC2",xlab="pcaComp1",ylab="pcaComp2",col="red") 
points(pc.comp1[class2],pc.comp2[class2],col="blue")

plot(pc.comp1[class1],pc.comp3[class1],main="PC1 vs PC3",xlab="pcaComp1",ylab="pcaComp2",col="red") 
points(pc.comp1[class2],pc.comp3[class2],col="blue")

plot(pc.comp2[class1],pc.comp3[class1],main="PC2 vs PC3",xlab="pcaComp1",ylab="pcaComp2",col="red") 
points(pc.comp2[class2],pc.comp3[class2],col="blue")


## Not sure how to figure out misclasssfication rate for Training Date Sets
## Not sure how to run the Testing Data through the model
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
##------------------------------------
## Fit LDA model
##------------------------------------

lda.fit <- lda(class~., data = seismic, subset = train)

##
## Predict back onto training data
##

lda.pred <- predict(lda.fit, seismic.train)
lda.class <- lda.pred$class

## Confusion matrix
confusion <- table(lda.class,seismic.train$class)
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])

## Sensitivity is very bad! Dramatically underpredict 1s
confusion
sensitivity
specificity

##
## Here is a function that could calculate overall error rates
## as a function of the posterior prediction probability threshhold
##

posterior <- lda.pred$posterior
truth <- seismic.train$class

mod.posterior <- function(posterior, truth, prob, dimension = length(train)){
  idx0 <- which(posterior[,1] > prob)
  idx1 <- (1:dimension)[-idx0]
  
  prediction <- rep(NA,dimension)
  prediction[idx0] = 0
  prediction[idx1] = 1
  
  mx <- cbind(prediction,truth,prediction-truth)
  
  confusion <- matrix(rep(NA,4), nrow = 2)
  correct <- which(mx[,3] == 0)
  confusion[1,1] <- length(which(mx[correct,1] == 0))
  confusion[2,2] <- length(which(mx[correct,1] == 1))
  confusion[1,2] <- length(which(mx[,3] == -1))
  confusion[2,1] <- length(which(mx[,3] == 1))
  
  sensitivity <- confusion[2,2]/sum(confusion[,2])
  specificity <- confusion[1,1]/sum(confusion[,1])
  error.rate <- (confusion[1,2] + confusion[2,1])/sum(confusion)
  c(sensitivity, specificity, error.rate)
}

prob.seq <- seq(.5,.98,by = .02)
output <- matrix(rep(NA,length(prob.seq)*2), ncol = 2)
colnames(output) <- c("Sensitivity", "Error.rate")

for(i in 1:length(prob.seq)){
  output[i,] <- mod.posterior(posterior,truth,prob.seq[i])[c(1,3)]
}

df <- as.data.frame(cbind(prob.seq,output))

ggplot(data = df, aes(x=prob.seq)) +
  geom_line(aes(y = Sensitivity, colour = "Sensitivity"), linetype = "dashed") +
  geom_line(aes(y = Error.rate, colour = "Error rate")) +
  scale_colour_manual(values=c("dark cyan", "dark grey"))
  


##
## Now try on test data
##

lda.pred <- predict(lda.fit, seismic.test)
lda.class <- lda.pred$class

## Confusion matrix
confusion <- table(lda.class,seismic.test$class)
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])

## Sensitivity is slightly worse here
confusion
sensitivity
specificity

posterior <- lda.pred$posterior
truth <- seismic.train$class
output.test <- matrix(rep(NA,length(prob.seq)*2), ncol = 2)
colnames(output.test) <- c("Sensitivity", "Error.rate")

for(i in 1:length(prob.seq)){
  output.test[i,] <- mod.posterior(posterior,truth,prob.seq[i])[c(1,3)]
}

df <- as.data.frame(cbind(prob.seq,output.test))

ggplot(data = df, aes(x=prob.seq)) +
  geom_line(aes(y = Sensitivity, colour = "Sensitivity"), linetype = "dashed") +
  geom_line(aes(y = Error.rate, colour = "Error rate")) +
  scale_colour_manual(values=c("dark cyan", "dark grey"))


#
# NOTE: We can modify LDA such by changing the RHS of P(class = 1|X=x) > 0.5 to something smaller
# This will yield a larger error rate, but with better identification of 1s, which is more important in this case
# Not sure at the moment how to search for optimal RHS, other than ROC curve, but we should definitely do this
#

##------------------------------------
## Fit QDA model
##------------------------------------

## Currently, can't perform QDA.  This is probably due to multicollinearity in the model
## (can't invert covariance matrix) but should be possible after variable selection

#qda.fit <- qda(class~., data = seismic, subset = train)

```

# Logistic Regression on the Training and Test Sets

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}

##------------------------------------
## Logistic Regression (Train)
##------------------------------------

seismic.train$class <- as.factor(seismic.train$class)
seismic.test$class <- as.factor(seismic.test$class)

glm.fit=glm(class~., data=seismic.train ,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit ,type="response")

```

The predictors that are significant in our logistic model are seismic, shift and gpuls.  The predictors nbumps6, nbumps7 and nbumps89 were removed as they did not provide any data.

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}

##--------------------------------------------
## Logistic Regression (TRAIN) Confusion\Roc
##--------------------------------------------
par(mfrow=c(1,1))

glm.pred=rep("0",1938)
glm.pred[glm.probs >.5]="1"
confusion <- table(glm.pred ,seismic.train$class)
mean(glm.pred==seismic.train$class)

# Confusion Table
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])

confusion
sensitivity
specificity

library(pROC)

# Setting up for Roc Curve
fit.glm.train <- glm(class~., seismic.train, family=binomial(link="logit"))
glm.link.scores.train <- predict(fit.glm.train, seismic.train, type="link")
glm.response.scores.train <- predict(fit.glm.train, seismic.train, type="response")
roc.Train <- roc(seismic.train$class, glm.response.scores.train, direction="<")

```

The diagonal elements of the confusion matrix indicate correct predictions,
while the off-diagonals represent incorrect predictions. Hence our model on the training data set correctly predicted that the seismic activity would be of no harzard on 1786 observations and that it would be of hazard on 0 observations, for a total of 1786 + 0 = 1786 correct predictions. The mean() function can be used to compute the fraction of hazards for which the prediction was correct. In this case, logistic regression correctly predicted the class of hazard 92 percent of the time.  The bad part about this 92 percent of the time is that it did not get any of our actual real hazards observations correct!!!

```{r, echo=FALSE, warning=FALSE, message=FALSE}

##--------------------------------------------
## Logistic Regression (TEST) Confusion\Roc
##--------------------------------------------

glm.fit=glm(class~., data=seismic.test ,family=binomial)
glm.probs=predict(glm.fit, type="response")

glm.pred=rep("0",646)
glm.pred[glm.probs >.5]="1"
confusion <- table(glm.pred, seismic.test$class)
mean(glm.pred==seismic.test$class)

# Confusion Table
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])

confusion
sensitivity
specificity

# Setting up for Roc Curve
fit.glm.test <- glm(class~., seismic.test, family=binomial(link="logit"))
glm.link.scores.test <- predict(fit.glm.test, seismic.test, type="link")
glm.response.scores.test <- predict(fit.glm.test, seismic.test, type="response")
roc.Test <- roc(seismic.test$class, glm.response.scores.test, direction="<")

# Plotting Roc Curve
plot.roc(roc.Train, col="blue", auc.polygon=TRUE,main="ROC Curve", xlab="False Positive Rate", ylab="True Positive Rate")
legend("bottomright", legend=c("Train", "Test"), col=c("blue", "red"), lwd=2)
plot.roc(roc.Test, col="red", lty=3, add=TRUE)

```

The diagonal elements of the confusion matrix indicate correct predictions,
while the off-diagonals represent incorrect predictions. Hence our model on the testing data set correctly predicted that the seismic activity would be of no harzard on 605 observations and that it would be hazardous on 2 observations, for a total of 602 + 2 = 604 correct predictions. The mean() function can be used to compute the fraction of seismic activity for which the prediction was correct. In this case, logistic regression correctly predicted class of hazard 93.5 % of the time.  However, again worrisome, is that the model miss 5 observations that were hazardous instances and 37 that were not hazardous.

Recall that the logistic regression model had only 3 predictors that were significant from an avaiable 19.  Perhaps by removing the variables that appear not to be helpful in predicting seismic hazard, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement [straight from the book]

## Random Notes on Roc Curve

This type of graph is called a Receiver Operating Characteristic curve (or ROC curve.) It is a plot of the true positive rate against the false positive rate for the different possible cutpoints of a diagnostic test.

An ROC curve demonstrates several things:

It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).  The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.  The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.  The slope of the tangent line at a cutpoint gives the likelihood ratio (LR) for that value of the test. You can check this out on the graph above. Recall that the LR for T4 < 5 is 52. This corresponds to the far left, steep portion of the curve. The LR for T4 > 9 is 0.2. This corresponds to the far right, nearly horizontal portion of the curve.  The area under the curve is a measure of text accuracy. This is discussed further in the next section.

The accuracy of the test depends on how well the test separates the group being tested into those with and without the disease in question. Accuracy is measured by the area under the ROC curve. An area of 1 represents a perfect test; an area of .5 represents a worthless test. A rough guide for classifying the accuracy of a diagnostic test is the traditional academic point system:

.90-1 = excellent (A)
.80-.90 = good (B)
.70-.80 = fair (C)
.60-.70 = poor (D)
.50-.60 = fail (F)

